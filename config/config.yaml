# config.yaml - User configuration for INTV
#
# This file contains all user-editable settings for the CLI and LLM/RAG system.
# Place this file in the config/ directory. Edit as needed to set defaults for your environment.
#
# For more information, see the README.md or project documentation.

# LLM API base URL (KoboldCpp, OpenAI)
llm_api_base: "http://localhost"

# LLM API key (if required)
# llm_api_key: "your-api-key-here"

# LLM API port (KoboldCpp default: 5001)
llm_api_port: 5001

# LLM provider: koboldcpp, openai
llm_provider: "koboldcpp"  # Options: koboldcpp, openai

# Model name or ID (using Q5_K_M for good balance of speed and quality)
model: "hf.co/unsloth/Phi-4-reasoning-plus-GGUF:Phi-4-reasoning-plus-Q5_K_M.gguf"

# LLM Configuration
llm:
  # LLM mode: embedded, external, hybrid
  mode: "embedded"
  
  # Max tokens for generation - use "auto" to match model's context window
  max_tokens: "auto"  # Options: "auto", integer (e.g., 1024, 2048)
  
  # Context size override - use "auto" for automatic detection
  context_size: "auto"  # Options: "auto", integer (e.g., 4096, 8192)
  
  # Generation parameters
  temperature: 0.7
  top_p: 0.9
  
  # Embedded LLM settings
  embedded:
    model: "auto"  # Auto-select based on hardware
    context_size: "auto"  # Options: "auto", integer (e.g., 2048, 4096, 8192)
    
  # External LLM settings
  external:
    provider: "koboldcpp"  # Options: koboldcpp, openai, ollama
    api_base: "http://localhost"
    api_port: 5001
    api_key: ""  # Set if required by provider
    model: "auto"
    timeout: 30
    context_size: "auto"  # Options: "auto", integer (e.g., 2048, 4096, 8192)

# RAG Configuration
rag:
  # RAG mode: embedded, external_tika, external_haystack
  mode: "embedded"
  
  # Embedded RAG settings
  embedded:
    # Model for embeddings (auto-selected based on system if not specified)
    model: "auto"  # Options: auto, hf.co/sentence-transformers/all-MiniLM-L6-v2, etc.
    chunk_size: 1000
    chunk_overlap: 100
    top_k: 5
    
  # External Tika settings
  external_tika:
    api_url: "http://localhost:9998"
    timeout: 30
    
  # External Haystack settings  
  external_haystack:
    api_url: "http://localhost:8000"
    api_key: ""
    timeout: 30

# Model download directory
model_dir: "models"

# Use external RAG service (true/false) - DEPRECATED, use rag.mode instead
external_rag: false

# Purge variables from the database on run (true/false)
purge_variables: false

# Audio Processing Configuration
audio:
  # Audio processing mode: embedded (local models), external (API services)
  mode: "embedded"
  
  # Voice Activity Detection (VAD) settings
  vad:
    # Enable VAD preprocessing for transcription
    enabled: true
    # Use pyannote VAD model (requires HuggingFace token)
    use_pyannote: true
    # VAD model to use
    model: "pyannote/segmentation-3.0"
    # VAD threshold parameters
    onset: 0.5
    offset: 0.5
    min_duration_on: 0.0
    min_duration_off: 0.0
    frame_duration: 0.02
    threshold: 0.01
  
  # Speaker Diarization settings
  diarization:
    # Enable speaker diarization
    enabled: true
    # Use pyannote diarization model (requires HuggingFace token)
    use_pyannote: true
    # Diarization model to use
    model: "pyannote/speaker-diarization-3.1"
    # Speaker constraints
    min_speakers: 1
    max_speakers: 10
    # Auto-detect number of speakers (null for auto)
    num_speakers: null
  
  # Audio Transcription settings
  transcription:
    # Whisper model selection based on hardware capabilities
    model: "auto"  # Options: auto, base, small, medium, large, large-v2, large-v3-turbo
    # Use faster-whisper for better performance
    use_faster_whisper: true
    # Language for transcription (null for auto-detection)
    language: null
    # Audio preprocessing
    sample_rate: 16000
    # Return word-level timestamps
    word_timestamps: true
  
  # Hardware-optimized model selection
  models:
    # VAD models by hardware tier
    vad_models:
      cpu_minimal: "pyannote/segmentation-3.0"
      cpu_low: "pyannote/segmentation-3.0"
      cpu_medium: "pyannote/segmentation-3.0"
      cpu_high: "pyannote/segmentation-3.0"
      gpu_low: "pyannote/segmentation-3.0"
      gpu_medium: "pyannote/segmentation-3.0"
      gpu_high: "pyannote/segmentation-3.0"
    
    # Diarization models by hardware tier
    diarization_models:
      cpu_minimal: "pyannote/speaker-diarization-3.1"
      cpu_low: "pyannote/speaker-diarization-3.1"
      cpu_medium: "pyannote/speaker-diarization-3.1"
      cpu_high: "pyannote/speaker-diarization-3.1"
      gpu_low: "pyannote/speaker-diarization-3.1"
      gpu_medium: "pyannote/speaker-diarization-3.1"
      gpu_high: "pyannote/speaker-diarization-3.1"
    
    # Whisper models by hardware tier (faster-whisper format)
    whisper_models:
      cpu_minimal: "faster-whisper/tiny"
      cpu_low: "faster-whisper/base"
      cpu_medium: "faster-whisper/small"
      cpu_high: "faster-whisper/medium"
      gpu_low: "faster-whisper/base"
      gpu_medium: "faster-whisper/medium"
      gpu_high: "faster-whisper/large-v3"
  
  # Live streaming settings
  streaming:
    # Enable live microphone processing
    enabled: true
    # Chunk duration for real-time processing
    chunk_duration: 2.0
    # Buffer size for continuous processing
    buffer_duration: 10.0
    # Continuous processing until silence detected
    continuous_until_silence: true
    # Silence threshold for stopping continuous processing
    silence_threshold: 2.0

#
# Multi-user login info is now stored in config/users.yaml.
# To enable multi-user support, edit config/users.yaml and add users as needed.
# The default_user above is ignored if users.yaml exists and is non-empty.
#
# You can remove the default_user block above if you want to require users.yaml for all logins.
#
# Add more global settings as needed below.
# For example:
# default_language: "en"
# enable_diarization: true
# enable_vad: true
# transcription_output_format: "json"
# rag_backend: "local"  # or "cloud"
