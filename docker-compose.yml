+networks:
  vLAN:
     external: true


services:
  intv_app:
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
    # erebusnyx/intv-app-gpu:latest
    image: docker.io/library/intv-app-gpu:latest
    container_name: intv_app
    restart: unless-stopped
    stdin_open: true
    tty: true
    env_file:
      - .env
    shm_size: '2gb'
    environment:
      # LLM and RAG config (override with your own env or .env file)
      LLM_API_PORT: ${LLM_API_PORT:-5001}
      LLM_PROVIDER: ${LLM_PROVIDER:-ollama}
      MODEL: ${MODEL:-hf.co/unsloth/Phi-4-reasoning-plus-GGUF:Q5_K_M}
      EXTERNAL_RAG: ${EXTERNAL_RAG:-false}  # Set to 'true' to enable external RAG, 'false' to disable
      PURGE_VARIABLES: ${PURGE_VARIABLES:-false}  # Set to 'true' to enable purging, 'false' to disable
      SESSION_COOKIE_SECURE: "true" # Force non-secure cookie for troubleshooting
      APP_ADMIN_USERNAME: ${APP_ADMIN_USERNAME:-admin}
      APP_ADMIN_PASSWORD: ${APP_ADMIN_PASSWORD:-admin}
    volumes:
      - ./config:/app/config
      - ./src:/app/src
      - ./scripts:/app/scripts
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - 3773:3773
    networks:
      vLAN:
        aliases:
          - intv_app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3773/api/v1/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/app/scripts/cloudflared-entrypoint.sh"]




