networks:
  vLAN:
     external: true


services:
  intv_app:
    build:
      context: .
      dockerfile: docker/Dockerfile.gpu
    # erebusnyx/intv-app-gpu:latest
    image: docker.io/library/intv-app-gpu:latest
    container_name: intv_app
    restart: unless-stopped
    stdin_open: true
    tty: true
    env_file:
      - .env
    shm_size: '2gb'
    environment:
      # LLM and RAG config (override with your own env or .env file)
      LLM_API_BASE: ${LLM_API_BASE:-http://localhost}
      LLM_API_KEY: ${LLM_API_KEY:-}
      LLM_API_PORT: ${LLM_API_PORT:-11434}
      LLM_PROVIDER: ${LLM_PROVIDER:-ollama}
      MODEL: ${MODEL:-hf.co/unsloth/Phi-4-reasoning-plus-GGUF:Q5_K_M}
      EXTERNAL_RAG: ${EXTERNAL_RAG:-false}
      PURGE_VARIABLES: ${PURGE_VARIABLES:-false}
      NAME: ${NAME:-admin}
      CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-0}
      SESSION_SECRET: ${SESSION_SECRET:-""}
      CF_AUD: ${CF_AUD}
      #APP_ADMIN_USER: ${APP_ADMIN_USER:-admin}
      #APP_ADMIN_PASS: ${APP_ADMIN_PASS:-password1234}
      # USE_CLOUDFLARE_TUNNEL: ${USE_CLOUDFLARE_TUNNEL:-0}  # Removed to ensure .env is used
    volumes:
      - ./config:/app/config
      - ./src:/app/src
      - ./requirements.txt:/app/requirements.txt
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - 3773:3773
    networks:
      vLAN:
        aliases:
          - intv_app
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3773/api/v1/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    entrypoint: ["/app/scripts/cloudflared-entrypoint.sh"]




