#!/usr/bin/env python3
"""
Test complete RAG-to-LLM pipeline with text generation
"""

import sys
import os
import tempfile
from pathlib import Path

# Add the intv module to the path
sys.path.insert(0, str(Path(__file__).parent / 'intv'))

def test_rag_to_llm_pipeline():
    """Test the complete RAG-to-LLM pipeline with text generation"""
    print("üß™ Testing Complete RAG-to-LLM Pipeline with Text Generation")
    print("=" * 80)
    
    try:
        from intv.rag_system import RAGSystem
        from intv.llm import HybridLLMProcessor
        from intv.rag import chunk_document
        
        # Create proper configuration with separate models
        config = {
            'llm': {
                'mode': 'embedded',
                'max_tokens': 'auto',
                'context_size': 'auto',
                'temperature': 0.7,
                'embedded': {
                    'model': 'auto'  # Will auto-select Phi-4 text generation model
                }
            },
            'rag': {
                'mode': 'embedded',
                'embedding_model': 'hf.co/sentence-transformers/multi-qa-mpnet-base-dot-v1',
                'chunk_size': 512,
                'chunk_overlap': 50,
                'max_chunks': 5
            }
        }
        
        print("1. Initializing RAG and LLM systems...")
        rag_system = RAGSystem(config)
        llm_processor = HybridLLMProcessor(config)
        
        # Create test document
        test_document = """
        Project Status Report - Q4 2024
        
        Executive Summary:
        The INTV project has made significant progress in Q4 2024. Key achievements include:
        - Implementation of RAG system with 99.3% accuracy
        - Integration of auto context detection for optimal token utilization
        - Support for GPU and CPU deployment scenarios
        - Comprehensive testing suite with 95% coverage
        
        Technical Highlights:
        - RAG processing: 6-10 chunks/second on GPU systems
        - Auto context detection: Sub-millisecond token calculation
        - Memory efficiency: Enhanced with automatic token management
        - Model support: GGUF, transformers, and external API integration
        
        Challenges:
        - Audio pipeline implementation remains incomplete
        - Web UI needs major overhaul for production readiness
        - Multi-user support requires architectural changes
        
        Next Quarter Goals:
        - Complete audio processing pipeline
        - Implement real-time collaboration features
        - Optimize performance for large-scale deployments
        """
        
        print("2. Processing document with RAG system...")
        
        # Create temporary file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(test_document)
            temp_path = f.name
        
        try:
            # Process with RAG
            chunks = chunk_document(temp_path, config=config)
            print(f"   ‚úì Document chunked into {len(chunks)} pieces")
            
            # Test RAG query
            query = "What are the key achievements and challenges mentioned in the report?"
            rag_results = rag_system.process_query(query)
            print(f"   ‚úì RAG query processed, found {len(rag_results.get('chunks', []))} relevant chunks")
            
            print("3. Testing LLM text generation...")
            
            # Test general summary generation
            summary = llm_processor.generate_general_summary(test_document)
            if summary and len(summary) > 50:
                print(f"   ‚úì General summary generated: {len(summary)} characters")
                print(f"   Preview: {summary[:100]}...")
            else:
                print(f"   ‚ö†Ô∏è  General summary generation may have issues: {summary}")
            
            # Test policy-driven summary
            policy_prompt = "Extract key achievements, challenges, and next steps from this project report."
            policy_result = llm_processor.generate_policy_summary(test_document, policy_prompt)
            
            if policy_result.get('success', False):
                print(f"   ‚úì Policy summary generated successfully")
                extracted_data = policy_result.get('extracted_data', {})
                print(f"   Extracted fields: {list(extracted_data.keys())}")
            else:
                print(f"   ‚ö†Ô∏è  Policy summary had issues: {policy_result.get('error', 'Unknown error')}")
            
            print("4. Testing RAG-enhanced LLM generation...")
            
            # Combine RAG results with LLM generation
            rag_context = "\n".join([chunk.get('content', '') for chunk in rag_results.get('chunks', [])])
            enhanced_prompt = f"""
            Based on the following context from the document:
            
            {rag_context}
            
            Question: {query}
            
            Please provide a comprehensive answer based on the context provided.
            """
            
            enhanced_summary = llm_processor.generate_general_summary(enhanced_prompt)
            if enhanced_summary and len(enhanced_summary) > 50:
                print(f"   ‚úì RAG-enhanced response generated: {len(enhanced_summary)} characters")
                print(f"   Preview: {enhanced_summary[:150]}...")
            else:
                print(f"   ‚ö†Ô∏è  RAG-enhanced generation may have issues: {enhanced_summary}")
            
            return True
            
        finally:
            os.unlink(temp_path)
        
    except Exception as e:
        print(f"‚ùå RAG-to-LLM pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_auto_context_with_generation():
    """Test auto context detection with actual text generation"""
    print(f"\nüß™ Testing Auto Context Detection with Text Generation")
    print("=" * 80)
    
    try:
        from intv.llm import EmbeddedLLM
        
        config = {
            'llm': {
                'embedded': {
                    'model': 'auto'
                },
                'max_tokens': 'auto',
                'context_size': 'auto'
            }
        }
        
        embedded_llm = EmbeddedLLM(config)
        
        test_prompts = [
            "Write a brief summary of machine learning.",
            "Explain the benefits of Retrieval Augmented Generation in 2-3 sentences.",
            "What are the key advantages of using GGUF models for local inference? Please provide a detailed explanation covering performance, memory usage, and compatibility aspects."
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}. Testing prompt ({len(prompt)} chars):")
            print(f"   Prompt: {prompt}")
            
            # Calculate auto tokens
            context_size = embedded_llm.get_context_window_size()
            auto_tokens = embedded_llm.calculate_auto_max_tokens(prompt)
            
            print(f"   Context window: {context_size:,} tokens")
            print(f"   Auto max tokens: {auto_tokens:,}")
            print(f"   Efficiency: {(auto_tokens/context_size)*100:.1f}% of context")
            
            # Test actual generation (if model supports it)
            if embedded_llm.llama_model:
                try:
                    response = embedded_llm.llama_model(
                        prompt,
                        max_tokens=min(auto_tokens, 100),  # Limit for testing
                        temperature=0.7,
                        stop=[".", "\n\n"]  # Stop at first sentence for testing
                    )
                    
                    if response and 'choices' in response:
                        generated_text = response['choices'][0]['text'].strip()
                        print(f"   ‚úì Generated: {generated_text[:100]}...")
                    else:
                        print(f"   ‚ö†Ô∏è  Generation response format unexpected: {response}")
                        
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Generation test skipped: {e}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Auto context with generation test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run complete RAG-to-LLM pipeline tests"""
    print("üöÄ INTV Complete RAG-to-LLM Pipeline Test")
    print("=" * 80)
    
    # Test 1: Complete RAG-to-LLM Pipeline
    test1_passed = test_rag_to_llm_pipeline()
    
    # Test 2: Auto Context with Generation
    test2_passed = test_auto_context_with_generation()
    
    # Summary
    print(f"\nüìä Test Results:")
    print(f"‚úÖ RAG-to-LLM Pipeline: {'PASSED' if test1_passed else 'FAILED'}")
    print(f"‚úÖ Auto Context + Generation: {'PASSED' if test2_passed else 'FAILED'}")
    
    if test1_passed and test2_passed:
        print(f"\nüéâ All tests passed! Complete RAG-to-LLM pipeline is working!")
        print(f"\nüìà The LLM pipeline has been successfully restored with:")
        print(f"   ‚Ä¢ Proper model separation (RAG uses embeddings, LLM uses text generation)")
        print(f"   ‚Ä¢ Auto context detection for optimal token usage")
        print(f"   ‚Ä¢ GGUF model support with llama.cpp backend")
        print(f"   ‚Ä¢ Integration between RAG retrieval and LLM generation")
        return 0
    else:
        print(f"\n‚ùå Some tests failed. Check the output above for details.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
